## Data Analyst and Data Science with SQL
` 1. Data Literacy `
---
Data literacy refers to the ability to read, understand, interpret, and use data effectively. 

 
**DIKW Pyramid** is a structured view from raw data to smart decision-making.

Data - Raw facts and numbers without context.

Facts - Verified and true pieces of data.

Information - Organized and meaningful data.

Knowledge - Information combined with understanding or experience.

Insights - Useful discoveries or patterns found through data analysis.

Wisdom - The ability to make smart decisions based on knowledge and insights.

Big Data - Extremely large and complex data sets that require advanced tools to process.

Metadata - Data about data; it describes other data.


Data can be categorized into two groups:

Structured data is organized in rows and columns.It is easy to store, search, and analyze. This kind of data is usually stored in databases.(Customer name, age)

Unstructured data has no fixed format.It is not stored in rows and columns.(Images and videos)

Data analysis is the process of collecting, cleaning, and studying data to find useful information.It helps people and businesses make better decisions based on facts.

Data Analysis Process is a step-by-step method used to understand and find meaning in data.

It includes 6 main steps:

1.	Ask – Define the question you want to answer
	
2.	Prepare – Collect and organize the data
	
3.	Process – Clean and check the data for errors
	
4.	Analyze – Explore the data to find patterns and insights
	
5.	Share – Present the results in a clear way (charts, reports)
	
6.	Act – Use the insights to make smart decisions


**Data Types**: 

1. Quantitative Data (Numerical Data): is numerical and used for calculations.
   
     Interval Data: Numerical data where the difference between values is meaningful, but there is no true zero point.
   
     Ratio Data: Numerical data that has all the properties of interval data, but with a true zero point, which allows for meaningful ratios.
   
2. Qualitative Data (Categorical Data): is descriptive and used for categorization.
   
     Nominal Data: Categories with no specific order.
   
     Ordinal Data: Categories that have a meaningful order or ranking but no consistent difference between them.
   
**Database** is a place where information is stored and organized so it can be easily found and used.

**Dataset**: A structured collection of data, often presented in rows and columns, like a table.

**Stages of Analytics**:

*Descriptive Analytics*: Focuses on understanding what has happened.

*Diagnostic Analytics*: Focuses on understanding why something happened.

*Predictive Analytics*: Focuses on anticipating what is likely to happen.

*Prescriptive Analytics*: Focuses on recommending actions to optimize outcomes.

**Data Visualization**: The graphical representation of data to make it easier to understand, such as charts, graphs, and maps.

**Population**: The population is the entire group of individuals or items that is the focus of a statistical study.

**Sample**: A sample is a subset of the population selected for analysis to make inferences about the entire group.

**Observation unit**: Observation unit is the individual entity or item on which data is collected and analyzed in a study.
	
	For example, a loan agreement in a credit portfolio is an observation unit.


**Central Tendency Measures**: These are values that summarize the data. Arithmetic mean, median, mode, and quartiles are measures of central tendency.
		For example, the average loan amount in a bank’s credit portfolio is a central tendency measure.
**Arithmetic Mean**: The arithmetic mean is the value obtained by adding all the values in a series and dividing by the number of values.

**Median**: The median is the value that divides a series into two equal parts when the series is arranged in ascending or descending order. It is the middle value in the ordered list.

If the series is symmetric, meaning the values in the dataset are close to each other, it is appropriate to use the arithmetic mean as the representative value. However, if the series is not symmetric, the median should be preferred. If the arithmetic mean and median values are close to each other, the series is considered symmetric.

**Mode**:


SELECT AVG(loan_amount) AS average_loan_amount
FROM loans;

**Quartiles**:
**Range**:

İnterval
Percentile


**Standard Deviation**:
**Variance**:
**Skewness**:
**Kurtosis**:




**Correlation**: A statistical measure that describes the extent to which two variables are related.

**Data Cleaning**: The process of preparing data by fixing or removing incorrect, incomplete, or duplicate data.

**Data collection**:

**Data storytelling**:

**Data Analysis Techniques**:There are numerous techniques used in data analysis, each with its unique purpose and application. Here, we will discuss some of the most commonly used techniques, including exploratory analysis, regression analysis, Monte Carlo simulation, factor analysis, cohort analysis, cluster analysis, time series analysis, and sentiment analysis.

**Exploratory analysis**:
**Regression analysis**:
**Factor analysis**:
**Monte Carlo simulation**:
**Cluster analysis**:
**Cohort analysis**:
**Time series analysis**:
**Sentiment analysis**:
**Cluster analysis**:



**Data Privacy**: Ensuring that personal or sensitive data is protected from unauthorized access.

2. Statistics and Mathematics

Descriptive statistic
Measures of Central Tendency
Measures of Dispersion
Frequency Distribution:
Interquartile Range

İnferential statistic
Sampling
Hypothesis testing
Coefficiant interval:
Significiant level (Alpha):
P-value:
Regression analysis:
Variance analysis (ANOVA):
Correlation and Covariance:
Chi-squared testing:

Random Number Generation (Excel)

Box plot, Histogram, Pie Diagram, Polygon Graph, Gaussian Normal Distribution Density graph

Frequency Table, Cumulative frequency, Grouping data, Expected Value calculationalgorithm

Probability Introduction (Cumulative Probability Function, Probability Mass Function)

Normal Probability (SPSS applied), Shapiro Wilk və Kolmogorov Smirnov testinin tətbiqi,
Shapiro Francia tətbiqi, Z score hesablanması və ehtimalın təsbiti (Standartlaşdırma), Excel
tətbiqləri

Student T distribution, Confidence İnterval Calculation process, Poisson Distribution,
Bernoulli Distribution, Binomial Distribution, Uniform Distribution

Sampling Dataset, Normality checking, Power Analysis, Sample Mean, Population mean,
Sample Variance, Sample Standard Deviation, Population Variance, Population Standard
Deviation

Hipotez anlayışı, Mərkəzi Limit Teoremi, One Sample T test nəzəri hissə, nümunələrlə
SPSS, STATA, EXCEL tətbiqi ilə

Independent Samples T test, Levene test nümunələrlə (Excel, SPSS, STATA tətbiqi ilə)

Paired Samples T test, nümunələrlə (Excel, SPSS, STATA tətbiqi ilə)

Korrelyasiya, Kovariance anlayışları (Excel, SPSS, STATA tətbiqi ilə)

Durbin Watson testi, R squared (Determinasiya əmsalı), Adjusted R squared,
Multicollinearlıq, Simple Linear Regression (Excel, STATA, SPSS tətbiqli)

Multiple Linear Regression, Gauss Markov şərtinin yoxlanılması, Least Squares Method
(Excel, STATA, SPSS tətbiqli)

One way ANOVA analizi (Excel, SPSS, STATA tətbiqli)

Two ways ANOVA analizi (Excel, SPSS, STATA tətbiqli)

Repated Measures ANOVA (SPSS, STATA tətbiqli)

Mixed ANOVA (SPSS, STATA tətbiqli)

MANOVA analysis

Chi Square test, Fisher`s Exact test, ANCOVA analizi

Weighted Regression, Binary Logistic Regression Analysis (SPSS tətbiqli)

Dummy Regression, Correlation Analysis (Pearson Correlation, Spearman Correlation,
Partial Correlation) (SPSS tətbiqli)

İyerarxik Regressiya, Parametric testlər (Mann Whitney U test, McNemar test, Sign test)
(SPSS tətbiqli)

Friedman, Kruskal Wallis, Wilxocon test (SPSS tətbiqli)

Cohen Kappa, Cronbach Alfa, Kendals testlərinin tətbiqi (SPSS tətbiqli)
İyerarxik klaster ilə cluster analysis (SPSS tətbiqli)
K means method ilə cluster analysis (SPSS tətbiqli)
Mediator Analysis
Factor Analysis
PCA analysis
Multinomial Logistic Regression
Poisson Regression, Gamma Regression, Binomial Regression

Data Preprocessing mərhələsi
Data Cleaning
Matchine learning
Model deployment

3. Microsoft Excel

4. SQL

5. NoSQl

6. PLSQL

A PL/SQL block has up to four different sections, only one of which is mandatory:

Header- Used only for named blocks. The header determines the way the named block or program must be called. Optional.

Declaration section-Identifies variables, cursors, and subblocks that are referenced in the execution and exception sections. Optional.

Execution section-Contains statements the PL/SQL runtime engine will execute at runtime. Manda‐ tory.

Exception section-Handles exceptions to normal processing (warnings and error conditions). Op‐ tional.

The general syntax of an anonymous PL/SQL block is as follows:

[ DECLARE ... declaration statements ... ] 
BEGIN ... one or more executable statements ... 
[ EXCEPTION... exception handler statements ... ]
 END;

Scope- In PL/SQL, variables, exceptions, modules, and a few other structures are local to the block that declares them. When the block stops executing, you can no longer reference any of these structures. 

A lexical unit in PL/SQL is any of the following:
• Identifier 
• Literal
• Delimiter
• Comment

Identifiers-
   An identifier is a name for a PL/SQL object, including any of the following:
• Constant or variable
• Exception
• Cursor
• Program name: procedure, function, package, object type, trigger, etc.
• Reserved word
• Label

Literals
A literal is a value that is not represented by an identifier; it is simply a value. 






7. BI tools - Power BI

Dax function


Tableau, Oracle BI

8. Python

print('Hello, Git!')
Data structure in Python

9. R

10. ML (Machine learning)

11. Deep Learning

12. NLP

13. Deployment and Cloud Platform

14. Projects
